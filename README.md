# Awesome-CVPR2024-AIGC[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of Papers and Codes for CVPR2024 AIGC

**Êï¥ÁêÜÊ±áÊÄª‰∏ã‰ªäÂπ¥CVPR AIGCÁõ∏ÂÖ≥ÁöÑËÆ∫ÊñáÂíå‰ª£Á†ÅÔºåÂÖ∑‰ΩìÂ¶Ç‰∏ã„ÄÇ**

**Ê¨¢ËøéstarÔºåforkÂíåPR~**

**Please feel free to star, fork or PR if helpful~**

**ÂèÇËÄÉÊàñËΩ¨ËΩΩËØ∑Ê≥®ÊòéÂá∫Â§Ñ**

CVPR2024ÂÆòÁΩëÔºö[https://cvpr.thecvf.com/Conferences/2024](https://cvpr.thecvf.com/Conferences/2024)

CVPRÂÆåÊï¥ËÆ∫ÊñáÂàóË°®Ôºöhttps://cvpr.thecvf.com/Conferences/2024/AcceptedPapers

ÂºÄ‰ºöÊó∂Èó¥Ôºö2024Âπ¥6Êúà17Êó•-6Êúà21Êó•

ËÆ∫ÊñáÊé•Êî∂ÂÖ¨Â∏ÉÊó∂Èó¥Ôºö2024Âπ¥2Êúà27Êó•

**„ÄêContents„Äë**

- [1.ÂõæÂÉèÁîüÊàê(Image Generation/Image Synthesis)](#1.ÂõæÂÉèÁîüÊàê)
- [2.ÂõæÂÉèÁºñËæëÔºàImage Editing)](#2.ÂõæÂÉèÁºñËæë)
- [3.ËßÜÈ¢ëÁîüÊàê(Video Generation/Image Synthesis)](#3.ËßÜÈ¢ëÁîüÊàê)
- [4.ËßÜÈ¢ëÁºñËæë(Video Editing)](#4.ËßÜÈ¢ëÁºñËæë)
- [5.3DÁîüÊàê(3D Generation/3D Synthesis)](#5.3DÁîüÊàê)
- [6.3DÁºñËæë(3D Editing)](#6.3DÁºñËæë)
- [7.Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(Multi-Modal Large Language Model)](#7.Â§ßËØ≠Ë®ÄÊ®°Âûã)
- [8.ÂÖ∂‰ªñÂ§ö‰ªªÂä°(Others)](#8.ÂÖ∂‰ªñ)

<a name="1.ÂõæÂÉèÁîüÊàê"></a>

# 1.ÂõæÂÉèÁîüÊàê(Image Generation/Image Synthesis)

### Accelerating Diffusion Sampling with Optimized Time Steps

- Paper: https://arxiv.org/abs/2402.17376
- Code: https://github.com/scxue/DM-NonUniform

### Adversarial Text to Continuous Image Generation
- Paper: https://openreview.net/forum?id=9X3UZJSGIg9
  
### Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder

### Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion

- Paper: https://arxiv.org/abs/2312.12471
- Code: https://github.com/zkawfanx/Atlantis

### CapHuman: Capture Your Moments in Parallel Universes

- Paper: https://arxiv.org/abs/2402.18078
- Code: https://github.com/VamosC/CapHuman

### Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation
- Paper: https://arxiv.org/abs/2311.15773
- Code:
  
### Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis

- Paper: https://arxiv.org/abs/2402.00627
- Code: https://github.com/YanzuoLu/CFLD

### CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation

- Paper: https://arxiv.org/abs/2310.01407
- Code: https://github.com/fast-codi/CoDi

### Condition-Aware Neural Network for Controlled Image Generation

### Countering Personalized Text-to-Image Generation with Influence Watermarks

### Cross Initialization for Personalized Text-to-Image Generation

- Paper: https://arxiv.org/abs/2312.15905
- Code: 

### Customization Assistant for Text-to-image Generation
- Paper: https://arxiv.org/abs/2312.03045


### DeepCache: Accelerating Diffusion Models for Free

- Paper: https://arxiv.org/abs/2312.00858
- Code: https://github.com/horseee/DeepCache

### DemoFusion: Democratising High-Resolution Image Generation With No $

- Paper: https://arxiv.org/abs/2311.16973
- Code: https://github.com/PRIS-CV/DemoFusion

### Diffusion-driven GAN Inversion for Multi-Modal Facial Image Generation

### DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models

- Paper: https://arxiv.org/abs/2402.19481
- Code: https://github.com/mit-han-lab/distrifuser

### Diversity-aware Channel Pruning for StyleGAN Compression

- Paper: 
- Code: https://github.com/jiwoogit/DCP-GAN

### Discriminative Probing and Tuning for Text-to-Image Generation

- Paper: https://www.arxiv.org/abs/2403.04321
- Code: https://github.com/LgQu/DPT-T2I

### DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization

- Paper: https://arxiv.org/abs/2402.09812
- Code: https://github.com/KU-CVLAB/DreamMatcher

### Dynamic Prompt Optimizing for Text-to-Image Generation

### ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations

- Paper: https://arxiv.org/abs/2312.04655
- Code: https://github.com/eclipse-t2i/eclipse-inference

### Efficient Dataset Distillation via Minimax Diffusion

- Paper: https://arxiv.org/abs/2311.15529
- Code: https://github.com/vimar-gu/MinimaxDiffusion

### ElasticDiffusion: Training-free Arbitrary Size Image Generation

- Paper: https://arxiv.org/abs/2311.18822
- Code: https://github.com/MoayedHajiAli/ElasticDiffusion-official

### EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models

- Paper: https://arxiv.org/abs/2401.04608
- Code: https://github.com/JingyuanYY/EmoGen

### Enabling Multi-Concept Fusion in Text-to-Image Models

- Paper: 
- Code: 

### Exact Fusion via Feature Distribution Matching for Few-shot Image Generation
- Paper: 
- Code: 

### FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition

### Generative Unlearning for Any Identity

- Paper: 
- Code: https://github.com/JJuOn/GUIDE

### HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances

- Paper: https://arxiv.org/abs/2403.01693

### High-fidelity Person-centric Subject-to-Image Synthesis

- Paper: https://arxiv.org/abs/2311.10329
- Code: https://github.com/CodeGoat24/Face-diffuser?tab=readme-ov-file

### InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning
- Paper: https://arxiv.org/abs/2304.03411
  
### InstanceDiffusion: Instance-level Control for Image Generation

- Paper: https://arxiv.org/abs/2402.03290
- Code: https://github.com/frank-xwang/InstanceDiffusion

### Instruct-Imagen: Image Generation with Multi-modal Instruction

- Paper: https://arxiv.org/abs/2401.01952
- Code: 

### Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models

- Paper: https://arxiv.org/abs/2306.00973
- Code: https://github.com/haoningwu3639/StoryGen

### InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model

- Paper: https://arxiv.org/abs/2312.05849
- Code: https://github.com/jiuntian/interactdiffusion

### Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models
- Paper: https://arxiv.org/abs/2308.15692

### Inversion-Free Image Editing with Natural Language

- Paper: https://arxiv.org/abs/2312.04965
- Code: https://github.com/sled-group/InfEdit

### JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation

### Learned representation-guided diffusion models for large-image generation
- Paper: https://arxiv.org/abs/2312.07330
  
### Learning Continuous 3D Words for Text-to-Image Generation

- Paper: https://arxiv.org/abs/2402.08654
- Code: https://github.com/ttchengab/continuous_3d_words_code/
### Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation
- Paper: https://arxiv.org/abs/2311.15841
- Code:

### Learning Multi-dimensional Human Preference for Text-to-Image Generation

### LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model

- Paper: 
- Code: https://github.com/ewrfcas/LeftRefill

### MACE: Mass Concept Erasure in Diffusion Models

- Paper: https://arxiv.org/abs/2402.05408
- Code: https://github.com/Shilin-LU/MACE

### MarkovGen: Structured Prediction for Efficient Text-to-Image Generation

Paper: https://arxiv.org/abs/2308.10997

### MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis

- Paper: https://arxiv.org/abs/2402.05408
- Code: https://github.com/limuloo/MIGC

### MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation

### On the Scalability of Diffusion-based Text-to-Image Generation

### Personalized Residuals for Concept-Driven Text-to-Image Generation

### PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding  

- Paper: https://arxiv.org/abs/2312.04461
- Code: https://github.com/TencentARC/PhotoMaker

### PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis

- Paper: 
- Code: https://github.com/cszy98/PLACE

### Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models

- Paper: https://arxiv.org/abs/2305.16223
- Code: https://github.com/SHI-Labs/Prompt-Free-Diffusion

### Relation Rectification in Diffusion Model

- Paper: 
- Code: https://github.com/WUyinwei-hah/RRNet

### Residual Denoising Diffusion Models

- Paper: https://arxiv.org/abs/2308.13712
- Code: https://github.com/nachifur/RDDM

### Rethinking FID: Towards a Better Evaluation Metric for Image Generation

- Paper: https://arxiv.org/abs/2401.09603
- Code: https://github.com/google-research/google-research/tree/master/cmmd

### Rich Human Feedback for Text-to-Image Generation

- Paper: https://arxiv.org/abs/2312.10240

### SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation

- Paper: https://arxiv.org/abs/2401.08053
- Code: 

### Self-correcting LLM-controlled Diffusion Models

- Paper: https://arxiv.org/abs/2311.16090
- Code: https://github.com/tsunghan-wu/SLD

### Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation

- Paper: https://arxiv.org/abs/2311.17216

### Shadow Generation for Composite Image Using Diffusion Model

- Paper: https://arxiv.org/abs/2308.09972
- Code: https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2

### Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models

- Paper: https://arxiv.org/abs/2312.04410
- Code: https://github.com/SHI-Labs/Smooth-Diffusion

### StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On

- Paper: https://arxiv.org/abs/2312.01725
- Code: https://github.com/rlawjdghek/StableVITON

### Style Aligned Image Generation via Shared Attention
- Paper: https://arxiv.org/abs/2312.02133
- Code: https://github.com/google/style-aligned/
  
### SVGDreamer: Text Guided SVG Generation with Diffusion Model

- Paper: https://arxiv.org/abs/2312.16476
- Code: https://github.com/ximinng/SVGDreamer

### Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting
- Paper: https://arxiv.org/abs/2310.08129
- Code: https://github.com/zzjchen/Tailored-Visions
  
### Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models
- Paper: https://arxiv.org/abs/2403.08381
- Code: https://github.com/PangzeCheung/SingDiffusion
  
### Taming Stable Diffusion for Text to 360‚àò Panorama Image Generation


### Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation
- Paper: https://arxiv.org/abs/2403.06247


### TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models

- Paper: https://arxiv.org/abs/2311.16503
- Code: https://github.com/ModelTC/TFMQ-DM

### TokenCompose: Grounding Diffusion with Token-level Supervision

- Paper: https://arxiv.org/abs/2312.03626
- Code: https://github.com/mlpc-ucsd/TokenCompose

### Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation
- Paper: https://arxiv.org/abs/2403.05239
  
### Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning

### UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs
- Paper: https://arxiv.org/abs/2311.09257
  
### UniGS: Unified Representation for Image Generation and Segmentation

- Paper: https://arxiv.org/abs/2312.01985
- Code: https://github.com/qqlu/Entity

### ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models

- Paper: https://arxiv.org/abs/2403.01807
- Code: https://github.com/facebookresearch/ViewDiff
  
### When StyleGAN Meets Stable Diffusion: a ùí≤+ Adapter for Personalized Image Generation
- Paper: https://arxiv.org/abs/2311.17461
- Code: https://github.com/csxmli2016/w-plus-adapter
  
### X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model

- Paper: https://arxiv.org/abs/2312.02238
- Code: https://github.com/showlab/X-Adapter


<a name="2.ÂõæÂÉèÁºñËæë"></a>

# 2.ÂõæÂÉèÁºñËæë(Image Editing)

### An Edit Friendly DDPM Noise Space: Inversion and Manipulations

- Paper: https://arxiv.org/abs/2304.06140
- Code: https://github.com/inbarhub/DDPM_inversion

### Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth

- Paper: 
- Code: https://github.com/Snowfallingplum/CSD-MT

### Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing

- Paper: https://arxiv.org/abs/2311.18608
- Code: https://github.com/HyelinNAM/ContrastiveDenoisingScore

### Deformable One-shot Face Stylization via DINO Semantic Guidance

- Paper: https://arxiv.org/abs/2403.00459
- Code: https://github.com/zichongc/DoesFS

### DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection

- Paper: 
- Code: https://github.com/HansSunY/DiffAM

### DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing

- Paper: https://arxiv.org/abs/2312.07409
- Code: https://github.com/Kevin-thu/DiffMorpher

### Doubly Abductive Counterfactual Inference for Text-based Image Editing

- Paper: https://arxiv.org/abs/2403.02981
- Code: https://github.com/xuesong39/DAC

### Edit One for All: Interactive Batch Image Editing

- Paper: https://arxiv.org/abs/2401.10219
- Code: https://github.com/thaoshibe/edit-one-for-all

### Face2Diffusion for Fast and Editable Face Personalization

- Paper: https://arxiv.org/abs/2403.05094
- Code: https://github.com/mapooon/Face2Diffusion

### Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation

- Paper: https://arxiv.org/abs/2312.10113
- Code: https://github.com/guoqincode/Focus-on-Your-Instruction

### FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation
- Paper: 
- Code: https://github.com/williamyang1991/FRESCO/tree/main
  
### Inversion-Free Image Editing with Natural Language

- Paper: hhttps://arxiv.org/abs/2312.04965
- Code: https://github.com/sled-group/InfEdit

### PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models

- Paper: https://arxiv.org/abs/2303.17546
- Code: https://github.com/Picsart-AI-Research/PAIR-Diffusion

### Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing

- Paper: https://arxiv.org/abs/2303.17546
- Code: https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE?tab=readme-ov-file

### PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models

- Paper: https://arxiv.org/abs/2312.13964
- Code: https://github.com/open-mmlab/PIA

### FreeDrag: Feature Dragging for Reliable Point-based Image Editing

- Paper: https://arxiv.org/abs/2307.04684
- Code: https://github.com/LPengYang/FreeDrag

### SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models

- Paper: https://arxiv.org/abs/2312.06739
- Code: https://github.com/TencentARC/SmartEdit

### Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer

- Paper: https://arxiv.org/abs/2312.09008
- Code: https://github.com/jiwoogit/StyleID

<a name="3.ËßÜÈ¢ëÁîüÊàê"></a>

# 3.ËßÜÈ¢ëÁîüÊàê(Video Generation/Video Synthesis)

### A Recipe for Scaling up Text-to-Video Generation with Text-free Videos

- Paper: https://arxiv.org/abs/2312.15770
- Code: https://tf-t2v.github.io/

### DisCo: Disentangled Control for Realistic Human Dance Generation

- Paper: https://arxiv.org/abs/2307.00040
- Code: https://github.com/Wangt-CN/DisCo

### MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model

- Paper: https://arxiv.org/abs/2311.16498
- Code: https://github.com/magic-research/magic-animate

### Make Your Dream A Vlog

- Paper: https://arxiv.org/abs/2401.09414
- Code: https://github.com/Vchitect/Vlogger

### Panacea: Panoramic and Controllable Video Generation for Autonomous Driving

- Paper: https://arxiv.org/abs/2311.16813
- Code: https://github.com/wenyuqing/panacea

### Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners

- Paper: https://arxiv.org/abs/2308.13712
- Code: https://github.com/yzxing87/Seeing-and-Hearing

### SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis

- Paper: https://arxiv.org/abs/2311.17590
- Code: https://github.com/ZiqiaoPeng/SyncTalk

### VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models

- Paper: https://arxiv.org/abs/2401.09047
- Code: https://github.com/AILab-CVC/VideoCrafter

### VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models

- Paper: https://arxiv.org/abs/2312.00845
- Code: https://github.com/HyeonHo99/Video-Motion-Customization

<a name="4.ËßÜÈ¢ëÁºñËæë"></a>

# 4.ËßÜÈ¢ëÁºñËæë(Video Editing)

### CoDeF: Content Deformation Fields for Temporally Consistent Video Processing

- Paper: https://arxiv.org/abs/2308.07926
- Code: https://github.com/qiuyu96/CoDeF

### VidToMe: Video Token Merging for Zero-Shot Video Editing

- Paper: https://arxiv.org/abs/2312.10656
- Code: https://github.com/lixirui142/VidToMe

### VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models

- Paper: https://arxiv.org/abs/2312.00845
- Code: https://github.com/HyeonHo99/Video-Motion-Customization

<a name="5.3DÁîüÊàê"></a>

# 5.3DÁîüÊàê(3D Generation/3D Synthesis)

### Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling

- Paper: https://arxiv.org/abs/2311.16096
- Code: https://github.com/lizhe00/AnimatableGaussians

### BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation

- Paper: https://arxiv.org/abs/2312.02136
- Code: https://github.com/zqh0253/BerfScene

### CAD: Photorealistic 3D Generation via Adversarial Distillation

- Paper: https://arxiv.org/abs/2312.06663
- Code: https://github.com/raywzy/CAD

### CityDreamer: Compositional Generative Model of Unbounded 3D Cities

- Paper: https://arxiv.org/abs/2309.00610
- Code: https://github.com/kxhit/EscherNet

### Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior

- Paper: https://arxiv.org/abs/2401.09050
- Code: https://github.com/sail-sg/Consistent3D

### DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis
- Paper: https://arxiv.org/abs/2303.14207
- Code: https://github.com/tangjiapeng/DiffuScene


### DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models

- Paper: https://arxiv.org/abs/2304.00916
- Code: https://github.com/yukangcao/DreamAvatar

### DreamComposer: Controllable 3D Object Generation via Multi-View Conditions

- Paper: https://arxiv.org/abs/2312.03611
- Code: https://github.com/yhyang-myron/DreamComposer

### DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior

- Paper: https://arxiv.org/abs/2312.06439
- Code: https://github.com/tyhuang0428/DreamControl

### EscherNet: A Generative Model for Scalable View Synthesis

- Paper: https://arxiv.org/abs/2402.03908
- Code: https://github.com/hzxie/city-dreamer

### GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models

- Paper: https://arxiv.org/abs/2310.08529
- Code: https://github.com/hustvl/GaussianDreamer

### GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation

- Paper: https://arxiv.org/abs/2401.04092
- Code: https://github.com/3DTopia/GPTEval3D

### Gaussian Shell Maps for Efficient 3D Human Generation

- Paper: https://arxiv.org/abs/2311.17857
- Code: https://github.com/computational-imaging/GSM

### HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D

- Paper: https://arxiv.org/abs/2312.15980
- Code: https://github.com/byeongjun-park/HarmonyView

### MoMask: Generative Masked Modeling of 3D Human Motions

- Paper: https://arxiv.org/abs/2312.00063
- Code: https://github.com/EricGuo5513/momask-codes

### EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion

- Paper: https://arxiv.org/abs/2312.06725
- Code: https://github.com/huanngzh/EpiDiff

### PEGASUS: Personalized Generative 3D Avatars with Composable Attributes

- Paper: https://arxiv.org/abs/2402.10636
- Code: https://github.com/snuvclab/pegasus

### RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D.

- Paper: https://arxiv.org/abs/2311.16918
- Code: https://github.com/modelscope/richdreamer

### SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors

- Paper: https://arxiv.org/abs/2311.17261
- Code: https://github.com/daveredrum/SceneTex

### SceneWiz3D: Towards Text-guided 3D Scene Composition

- Paper: https://arxiv.org/abs/2312.08885
- Code: https://github.com/zqh0253/SceneWiz3D

### SemCity: Semantic Scene Generation with Triplane Diffusion
- Paper: https://arxiv.org/abs/2403.07773
- Code: https://github.com/zoomin-lee/SemCity?tab=readme-ov-file

### Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior

- Paper: https://arxiv.org/abs/2312.06655
- Code: https://github.com/liuff19/Sherpa3D

### Text-to-3D using Gaussian Splatting

- Paper: https://arxiv.org/abs/2309.16585
- Code: https://github.com/gsgen3d/gsgen

### ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models

- Paper: https://arxiv.org/abs/2312.01305
- Code: https://github.com/ubc-vision/vivid123


<a name="6.3DÁºñËæë"></a>

# 6.3DÁºñËæë(3D Editing)

### GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting

- Paper: https://arxiv.org/abs/2311.14521
- Code: https://github.com/buaacyw/GaussianEditor


<a name="7.Â§ßËØ≠Ë®ÄÊ®°Âûã"></a>

# 7.Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã(Multi-Modal Large Language Models)

### Alpha-CLIP: A CLIP Model Focusing on Wherever You Want

- Paper: https://arxiv.org/abs/2312.03818
- Code: https://github.com/SunzeY/AlphaCLIP

### Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding

- Paper: https://arxiv.org/abs/2311.08046
- Code: https://github.com/PKU-YuanGroup/Chat-UniVi

### Efficient Stitchable Task Adaptation

- Paper: https://arxiv.org/abs/2311.17352
- Code: https://github.com/ziplab/Stitched_LLaMA

### GPT4Point: A Unified Framework for Point-Language Understanding and Generation

- Paper: https://arxiv.org/abs/2312.02980
- Code: https://github.com/Pointcept/GPT4Point

### InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks

- Paper: https://arxiv.org/abs/2312.14238
- Code: https://github.com/OpenGVLab/InternVL

### LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge

- Paper: https://arxiv.org/abs/2311.11860
- Code: https://github.com/rshaojimmy/JiuTian

### LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning

- Paper: https://arxiv.org/abs/2311.18651
- Code: https://github.com/Open3DA/LL3DA

### Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding

- Paper: https://arxiv.org/abs/2311.16922
- Code: https://github.com/DAMO-NLP-SG/VCD

### OneLLM: One Framework to Align All Modalities with Language

- Paper: https://arxiv.org/abs/2312.03700
- Code: https://github.com/csuhan/OneLLM

### OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation

- Paper: https://arxiv.org/abs/2402.19479
- Code: https://github.com/shikiw/OPERA

### Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers

- Paper: https://arxiv.org/abs/2311.17911
- Code: https://github.com/snap-research/Panda-70M

### PixelLM: Pixel Reasoning with Large Multimodal Model

- Paper: https://arxiv.org/abs/2312.02228
- Code: https://github.com/MaverickRen/PixelLM

### Prompt Highlighter: Interactive Control for Multi-Modal LLMs

- Paper: https://arxiv.org/abs/2312.04302
- Code: https://github.com/dvlab-research/Prompt-Highlighter

### Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models

- Paper: https://arxiv.org/abs/2311.06783
- Code: https://github.com/Q-Future/Q-Instruct

### SEED-Bench: Benchmarking Multimodal Large Language Models

- Paper: https://arxiv.org/abs/2311.17092
- Code: https://github.com/AILab-CVC/SEED-Bench

### VBench: Comprehensive Benchmark Suite for Video Generative Models

- Paper: https://arxiv.org/abs/2311.17982
- Code: https://github.com/Vchitect/VBench

### VideoChat: Chat-Centric Video Understanding
- Paper: https://arxiv.org/abs/2305.06355
- Code: https://github.com/OpenGVLab/Ask-Anything

### ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts

- Paper: https://arxiv.org/abs/2312.00784
- Code: https://github.com/mu-cai/ViP-LLaVA

### ViT-Lens: Towards Omni-modal Representations

- Paper: https://github.com/TencentARC/ViT-Lens
- Code: https://arxiv.org/abs/2308.10185

  
<a name="8.ÂÖ∂‰ªñ"></a>

# 8.ÂÖ∂‰ªñÂ§ö‰ªªÂä°(Others)

### AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error

- Paper: https://arxiv.org/abs/2401.17879
- Code: https://github.com/jonasricker/aeroblade

### EvalCrafter: Benchmarking and Evaluating Large Video Generation Models

- Paper: https://arxiv.org/abs/2310.11440
- Code: https://github.com/evalcrafter/EvalCrafter



<font color=red size=5>ÊåÅÁª≠Êõ¥Êñ∞~</font>

# ÂèÇËÄÉ

[CVPR 2024 ËÆ∫ÊñáÂíåÂºÄÊ∫êÈ°πÁõÆÂêàÈõÜ(Papers with Code)](https://github.com/amusi/CVPR2024-Papers-with-Code)

# Áõ∏ÂÖ≥Êï¥ÁêÜ

- [Awesome-AIGC-Research-Groups](https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups)
- [Awesome-Low-Level-Vision-Research-Groups](https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups)
- [Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision)
- [Awesome-ECCV2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision)
